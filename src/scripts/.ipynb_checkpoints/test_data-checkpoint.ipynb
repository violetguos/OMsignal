{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 1, 3750)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', '..')))\n",
    "\n",
    "\n",
    "# import legacy code from team5\n",
    "from src.legacy.TeamB1pomt5.code.omsignal.utils.dataloader_utils import get_dataloader\n",
    "from src.legacy.TeamB1pomt5.code.omsignal.utils.dataloader_utils import import_train_valid, OM_dataset\n",
    "from src.scripts.dataloader_utils import import_OM\n",
    "from src.legacy.TeamB1pomt5.code.omsignal.utils.augmentation import RandomCircShift, RandomDropoutBurst, RandomNegate, RandomReplaceNoise\n",
    "from src.legacy.TeamB1pomt5.code.omsignal.utils.preprocessor import Preprocessor\n",
    "from src.legacy.TeamB1pomt5.code.omsignal.utils.pytorch_utils import log_training, get_id_mapping, map_ids\n",
    "from src.legacy.TeamB1pomt5.code.config import LOG_DIR, MODELS_DIR\n",
    "\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "\n",
    "from src.legacy.TeamB1pomt5.code.omsignal.utils.dataloader_utils import get_dataloader\n",
    "from src.legacy.TeamB1pomt5.code.config import MODELS_DIR\n",
    "from src.legacy.TeamB1pomt5.code.omsignal.utils.pytorch_utils import log_training\n",
    "\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "########\n",
    "\n",
    "from torch.nn.functional import log_softmax\n",
    "\n",
    "class HLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = F.softmax(x, dim=1) * F.log_softmax(x, dim=1)\n",
    "        b = -1.0 * b.sum()\n",
    "        return b\n",
    "\n",
    "\n",
    "def entropy_term(outputs,requires_grad):\n",
    "    \"\"\"\n",
    "    Function for computing the entropy term of the loss function\n",
    "    :param outputs: outputs on which to compute the NLL\n",
    "    :return: loss term\n",
    "    \"\"\"\n",
    "    loss = -1. * torch.mean(torch.sum(((outputs * torch.exp(outputs))), dim=1), dim=0)\n",
    "    loss = torch.autograd.Variable(loss, requires_grad)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def fetch_dataloaders(train_data, train_labels, valid_data, valid_labels, batch_size=50, transform=None): #No need with train_ID_CNN\n",
    "    \"\"\"\n",
    "    fetch_dataloders is a function which creates the dataloaders required for data training\n",
    "    :param train_data: n samples x 1 channel x m dimensions (3750 + 4/1)\n",
    "    :param valid_data:\n",
    "    :param unlabeled_data:\n",
    "    :return: dataloaders of input data\n",
    "    \"\"\"\n",
    "    train_loader = get_dataloader(train_data, train_labels, transform, batch_size=batch_size)\n",
    "    valid_loader = get_dataloader(valid_data, valid_labels, transform, batch_size=batch_size) #Only uses original validation data\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "def training_loop(training_dataloader, validation_dataloader, model):\n",
    "    \"\"\"\n",
    "\n",
    "    :param training_dataloader:\n",
    "    :param validation_dataloader:\n",
    "    :param model:\n",
    "    :return: model\n",
    "    \"\"\"\n",
    "    return model\n",
    "\n",
    "def get_map_ids(y_train,y_valid):\n",
    "    mapping = get_id_mapping(y_train[:,0])\n",
    "    y_train[:,0] = map_ids(y_train[:,0], mapping)\n",
    "    y_valid[:,0] = map_ids(y_valid[:,0], mapping)\n",
    "    return y_train,y_valid\n",
    "\n",
    "# Seeding\n",
    "np.random.seed(23)\n",
    "\n",
    "# Configure for GPU (or not)\n",
    "gpu_avail = torch.cuda.is_available()\n",
    "\n",
    "# cluster = False\n",
    "print('GPU available: {}'.format(gpu_avail))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "###########\n",
    "# move all to a single function in utils\n",
    "########################\n",
    "\n",
    "# Import the data but only ID labels, concatenating train and valid sets\n",
    "X_train, X_valid, y_train, y_valid = import_train_valid('ids', cluster=gpu_avail)\n",
    "\n",
    "# Remapping IDs\n",
    "y_train,y_valid = get_map_ids(y_train,y_valid)\n",
    "\n",
    "\n",
    "# Preprocess the data (moved this here since we wont use a dataloader for ranking predictions)\n",
    "preprocess = Preprocessor()\n",
    "preprocess.to(device)\n",
    "\n",
    "X_train = preprocess(torch.from_numpy(X_train)).numpy()\n",
    "X_valid = preprocess(torch.from_numpy(X_valid)).numpy()\n",
    "\n",
    "\n",
    "#################################3\n",
    "\n",
    "# Add transformations\n",
    "trsfrm = transforms.RandomChoice([RandomCircShift(0.5), RandomNegate(0.5), \\\n",
    "    RandomReplaceNoise(0.5), RandomDropoutBurst(0.5)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bs =12\n",
    "\n",
    "train_loader, valid_loader = fetch_dataloaders(X_train, y_train, X_valid, y_valid,bs, transform=trsfrm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "def norm(dim):\n",
    "    return nn.GroupNorm(min(32, dim), dim)\n",
    "\n",
    "\n",
    "class ConcatConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\n",
    "        super(ConcatConv2d, self).__init__()\n",
    "        module = nn.ConvTranspose2d if transpose else nn.Conv2d\n",
    "        self._layer = module(\n",
    "            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\n",
    "            bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "\n",
    "        tt = torch.ones_like(x[:, :1, :, :]) * t\n",
    "        ttx = torch.cat([tt, x], 1)\n",
    "        return self._layer(ttx)\n",
    "\n",
    "\n",
    "\n",
    "class ConcatConv1d(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\n",
    "        super(ConcatConv1d, self).__init__()\n",
    "        module = nn.ConvTranspose1d if transpose else nn.Conv1d\n",
    "        self._layer = module(\n",
    "            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\n",
    "            bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        tt = torch.ones_like(x) * t\n",
    "        ttx = torch.cat([tt, x], 1)\n",
    "        return self._layer(ttx)\n",
    "\n",
    "class ODEfunc(nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(ODEfunc, self).__init__()\n",
    "        self.norm1 = norm(dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\n",
    "        self.norm2 = norm(dim)\n",
    "        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\n",
    "        self.norm3 = norm(dim)\n",
    "        self.nfe = 0\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        self.nfe += 1\n",
    "        out = self.norm1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(t, out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(t, out)\n",
    "        out = self.norm3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ODEBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, odefunc):\n",
    "        super(ODEBlock, self).__init__()\n",
    "        self.odefunc = odefunc\n",
    "        self.integration_time = torch.tensor([0, 1]).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.integration_time = self.integration_time.type_as(x)\n",
    "        out = odeint(self.odefunc, x, self.integration_time, rtol=1e-5, atol=1e-5)\n",
    "        return out[1]\n",
    "\n",
    "    @property\n",
    "    def nfe(self):\n",
    "        return self.odefunc.nfe\n",
    "\n",
    "    @nfe.setter\n",
    "    def nfe(self, value):\n",
    "        self.odefunc.nfe = value\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n",
    "        return x.view(-1, shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: [    0}/{10000] Train Loss: 50.2386 Acc: 0.012500 ; Valid Loss: 48.688109 Acc: 0.031250\n",
      " Epoch: [    1}/{10000] Train Loss: 49.3964 Acc: 0.025000 ; Valid Loss: 48.596215 Acc: 0.031250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-712d3448d26b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;31m# Update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=2e-4, betas=(.5, .999), weight_decay=0)\n",
    "\n",
    "\n",
    "epochs = 10000\n",
    "\n",
    "temp_flag = True\n",
    "new_data = []\n",
    "new_labels = []\n",
    "\n",
    "task_num=0\n",
    "task_type=\"Classification\"\n",
    "train_dataloader = train_loader\n",
    "valid_dataloader = valid_loader\n",
    "loss_func = loss_function\n",
    "entropy = True\n",
    "num_epochs=epochs\n",
    "save_name=\"TestModel\"\n",
    "preprocess = Preprocessor()\n",
    "preprocess.to(device)\n",
    "if task_type not in [\"Regression\", \"Classification\", \"Ranking\"]:\n",
    "    raise ValueError(\"task_type must be in ['Regression', 'Classification' , 'Ranking']\")\n",
    "\n",
    "# Set up losses and accuracies\n",
    "train_losses, val_losses = [], []\n",
    "if task_type != \"Regression\":\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "# Variable to save save the best model\n",
    "max_valid_acc = 0\n",
    "s_m_values = [];\n",
    "loss_values = [];\n",
    "acc_values = []\n",
    "acc = 0\n",
    "large = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    \n",
    "    total, correct = 0, 0\n",
    "    \n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (data, label) in enumerate(train_dataloader):\n",
    "        data, label = data.float().to(device), label[:, task_num].to(device)\n",
    "        \n",
    "        label = label.long()\n",
    "\n",
    "        if not large:   \n",
    "            if epoch%5==0:\n",
    "                data = data[:,:,:750]\n",
    "                if large:\n",
    "                    large = False\n",
    "                else:\n",
    "                    large = True\n",
    "            elif epoch%4 ==0:\n",
    "                data = data[:,:,3000:]\n",
    "       \n",
    "            elif epoch%3==0:\n",
    "                data = data[:,:,2250:3000]\n",
    "            elif epoch%2==0:\n",
    "                data = data[:,:,1500:2250]\n",
    "            elif epoch%1==0:\n",
    "                data = data[:,:,750:1500]\n",
    "        \n",
    "        X = preprocess(data.cpu()).numpy()\n",
    "        fft = torch.Tensor(np.fft.rfft(X, axis = 2).astype(np.float32)).to(device)\n",
    "        \n",
    "        \n",
    "        noise1 = torch.randn_like(data)\n",
    "        \n",
    "\n",
    "        outputs = model(data,fft)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = loss_func(outputs, label)\n",
    "        loss_values.append(loss)\n",
    "\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "  \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == label).sum().item()\n",
    "        total += label.size(0)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        acc_values.append(acc)\n",
    "        \n",
    "    t_loss = running_loss;\n",
    "\n",
    "    acc_statement = ''\n",
    "    acc = correct / total\n",
    "    train_accuracies.append(acc)\n",
    "    acc_statement = '\\t Accuracy: {0:.2f}'.format(acc)\n",
    "        \n",
    "    train_losses.append(running_loss)\n",
    "    t_acc = acc;\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Validation\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total, correct = 0, 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (data, label) in enumerate(valid_dataloader):\n",
    "        \n",
    "        data, label = data.float().to(device), label[:, task_num].to(device)\n",
    "        \n",
    "        if task_type == \"Classification\":\n",
    "            label = label.long()\n",
    "        else:\n",
    "            label = label.float()\n",
    "        \n",
    "        X = preprocess(data.cpu()).numpy()\n",
    "        fft = torch.Tensor(np.fft.rfft(X, axis = 2).astype(np.float32)).to(device)\n",
    "        noise1 = torch.randn_like(data)\n",
    "        \n",
    "        \n",
    "#         outputs = model(input+noise1)\n",
    "#         catin = net(data,fft)\n",
    "        outputs = model(data,fft)\n",
    "        \n",
    "        loss = loss_func(outputs, label)\n",
    "#         if entropy:\n",
    "#             ent = entropy_term(outputs,True)\n",
    "#             loss += ent\n",
    "\n",
    "        if task_type == \"Classification\":\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == label).sum().item()\n",
    "            total += label.size(0)\n",
    "      \n",
    "        running_loss += loss.item()\n",
    "    v_loss = running_loss;\n",
    "    acc_statement = ''\n",
    "    if task_type != \"Regression\":\n",
    "        acc = correct / total\n",
    "        val_accuracies.append(acc)\n",
    "        acc_statement = '\\t Accuracy: {0:.2f}'.format(acc)\n",
    "\n",
    "    if acc >= max_valid_acc:\n",
    "        max_valid_acc = acc\n",
    "        if save_name != None:\n",
    "            torch.save(model.state_dict(), os.path.join(MODELS_DIR, '{}'.format(save_name)))\n",
    "\n",
    "\n",
    "    v_acc = acc;\n",
    "    val_losses.append(running_loss)\n",
    "\n",
    "\n",
    "\n",
    "#     sys.stdout.write(\"\\r Epoch: [%5d}/{%5d] Train Loss: %.4f Acc: %4f ; Valid Loss: %4f Acc: %4f\" %(epoch, num_epochs,t_loss,t_acc,0,0))\n",
    "\n",
    "    sys.stdout.write(\"\\r Epoch: [%5d}/{%5d] Train Loss: %.4f Acc: %4f ; Valid Loss: %4f Acc: %4f\" %(epoch, num_epochs,t_loss,t_acc,v_loss,v_acc))\n",
    "    sys.stdout.flush()\n",
    "    print(\"\\r Epoch: [%5d}/{%5d] Train Loss: %.4f Acc: %4f ; Valid Loss: %4f Acc: %4f\" %(epoch, num_epochs,t_loss,t_acc,v_loss,v_acc))\n",
    "# Reload the best model we saved\n",
    "# if save_name != None:\n",
    "#     model.load_state_dict(torch.load(os.path.join(MODELS_DIR, '{}'.format(save_name))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.downsample.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNet, self).__init__()\n",
    "\n",
    "        self.signet_full  = DownSample()\n",
    "        self.fftnet_full = DownSample2()\n",
    "        self.signet_sub  = DownSample3()\n",
    "        self.fftnet_sub = DownSample4()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        if x.size()[2]==750:\n",
    "            x = self.signet_sub(x)\n",
    "            y = self.fftnet_sub(y)\n",
    "        else:\n",
    "            x = self.signet_full(x)\n",
    "            y = self.fftnet_full(y)\n",
    "        xy = torch.cat((x,y),2)\n",
    "        out = xy.view(-1,128,5,5)\n",
    "        return out\n",
    "    \n",
    "class ODEModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ODEModel, self).__init__()\n",
    "        \n",
    "        self.downsample = MNet()\n",
    "        self.feature_layers = ODEBlock(ODEfunc(128))\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            norm(128),\n",
    "            nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(128, 32)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x,y):\n",
    "        out = self.downsample(x,y)\n",
    "        out = self.feature_layers(out)\n",
    "        out = self.fc_layers(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# net = MNet()\n",
    "# net.to(device)\n",
    "model = ODEModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample4(nn.Module):\n",
    "\n",
    "    def __init__(self,conv1_num_filters=64, conv2_num_filters=128, \\\n",
    "        conv_ksize=16, conv_stride=1, conv_padding=2, \\\n",
    "        pool_ksize=32, pool_stride=1, pool_padding=2 ):\n",
    "        super(DownSample4, self).__init__()\n",
    "  \n",
    "        # Set hyperparameters needed in forward\n",
    "        self.conv1_num_filters, self.conv2_num_filters= conv1_num_filters, conv2_num_filters\n",
    "\n",
    "        # Define layers\n",
    "        #   Conv 1\n",
    "        self.conv1 = nn.Conv1d(1, self.conv1_num_filters, \\\n",
    "                               kernel_size=conv_ksize*2, stride=conv_stride+2, padding=conv_padding)\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        self.conv1_bn = nn.BatchNorm1d(self.conv1_num_filters)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=pool_ksize*2, stride=pool_stride+1, padding=pool_padding)\n",
    "\n",
    "        #   Conv2\n",
    "        self.conv2 = nn.Conv1d(self.conv1_num_filters, self.conv2_num_filters, \\\n",
    "                               kernel_size=conv_ksize, stride=conv_stride+1, padding=conv_padding)\n",
    "\n",
    "        self.lr = nn.LeakyReLU(0.02)\n",
    "        self.drop = nn.Dropout(0.15)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.lr(self.conv1_bn(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DownSample3(nn.Module):\n",
    "\n",
    "    def __init__(self,conv1_num_filters=128, conv_ksize=32, conv_stride=3, conv_padding=1, \\\n",
    "        pool_ksize=32, pool_stride=2, pool_padding=0):\n",
    "        \n",
    "        super(DownSample3, self).__init__()\n",
    "  \n",
    "        # Set hyperparameters needed in forward\n",
    "        self.conv1_num_filters = conv1_num_filters\n",
    "\n",
    "        # Define layers\n",
    "        #   Conv 1\n",
    "        self.conv1 = nn.Conv1d(1, self.conv1_num_filters, \\\n",
    "                               kernel_size=conv_ksize*2, stride=conv_stride+1, padding=conv_padding,dilation=1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        self.conv1_bn = nn.BatchNorm1d(self.conv1_num_filters)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=pool_ksize*2, stride=pool_stride+1, padding=pool_padding,dilation=2)\n",
    "\n",
    "        \n",
    "        #relu and dropout\n",
    "        self.drop = nn.Dropout(0.15)\n",
    "        self.lr = nn.LeakyReLU(0.02)     \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.lr(self.conv1_bn(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "\n",
    "    def __init__(self,conv1_num_filters=16, conv2_num_filters=32, conv3_num_filters=128, \\\n",
    "        conv_ksize=32, conv_stride=1, conv_padding=1, \\\n",
    "        pool_ksize=32, pool_stride=1, pool_padding=0, \\\n",
    "        num_linear=128,p=0.5):\n",
    "        super(DownSample, self).__init__()\n",
    "        \n",
    "        # Set hyperparameters needed in forward\n",
    "        self.conv1_num_filters, self.conv2_num_filters, self.conv3_num_filters= conv1_num_filters, conv2_num_filters,conv3_num_filters\n",
    "\n",
    "        # Define layers\n",
    "        #   Conv 1\n",
    "        self.conv1 = nn.Conv1d(1, self.conv1_num_filters, \\\n",
    "                               kernel_size=conv_ksize*2, stride=conv_stride+1, padding=conv_padding,dilation=1)\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        self.conv1_bn = nn.BatchNorm1d(self.conv1_num_filters)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=pool_ksize*2, stride=pool_stride+1, padding=pool_padding,dilation=2)\n",
    "\n",
    "        #   Conv2\n",
    "        self.conv2 = nn.Conv1d(self.conv1_num_filters, self.conv2_num_filters, \\\n",
    "                               kernel_size=conv_ksize*2, stride=conv_stride+1, padding=conv_padding,dilation=2)\n",
    "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        self.conv2_bn = nn.BatchNorm1d(self.conv2_num_filters)\n",
    "        \n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=pool_ksize, stride=pool_stride+1, padding=pool_padding,dilation=2)\n",
    "\n",
    "        # Conv3\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(self.conv2_num_filters, self.conv3_num_filters, \\\n",
    "                                kernel_size=conv_ksize, stride=conv_stride+1, padding=conv_padding,dilation=4)\n",
    "        \n",
    "        #relu and dropout\n",
    "        self.lr = nn.LeakyReLU(0.02)\n",
    "        self.drop = nn.Dropout(0.15)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.lr(self.conv1_bn(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lr(self.conv2_bn(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "    \n",
    "class DownSample2(nn.Module):\n",
    "\n",
    "    '''\n",
    "    To be used as a submodule in more complex models --\n",
    "    just a classification using a CNN.\n",
    "    '''\n",
    "    def __init__(self,conv1_num_filters=16, conv2_num_filters=32, conv3_num_filters=128, \\\n",
    "        conv_ksize=32, conv_stride=1, conv_padding=1, \\\n",
    "        pool_ksize=32, pool_stride=1, pool_padding=1):\n",
    "        super(DownSample2, self).__init__()\n",
    "        self.lr = nn.LeakyReLU(0.02)     \n",
    "  \n",
    "        # Set hyperparameters needed in forward\n",
    "        self.conv1_num_filters, self.conv2_num_filters, self.conv3_num_filters= conv1_num_filters, conv2_num_filters,conv3_num_filters\n",
    "\n",
    "        # Define layers\n",
    "        \n",
    "        #   Conv 1\n",
    "        self.conv1 = nn.Conv1d(1, self.conv1_num_filters, \\\n",
    "                               kernel_size=conv_ksize*2, stride=conv_stride+2, padding=conv_padding)\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        self.conv1_bn = nn.BatchNorm1d(self.conv1_num_filters)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=pool_ksize*2, stride=pool_stride+1, padding=pool_padding)\n",
    "\n",
    "        #   Conv2\n",
    "        self.conv2 = nn.Conv1d(self.conv1_num_filters, self.conv2_num_filters, \\\n",
    "                               kernel_size=conv_ksize, stride=conv_stride+1, padding=conv_padding)\n",
    "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        self.conv2_bn = nn.BatchNorm1d(self.conv2_num_filters)\n",
    "        \n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=pool_ksize, stride=pool_stride+1, padding=pool_padding)\n",
    "\n",
    "        # Conv3\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(self.conv2_num_filters, self.conv3_num_filters, \\\n",
    "                                kernel_size=conv_ksize, stride=conv_stride+1, padding=conv_padding)\n",
    "        \n",
    "        #leaky relu and dropout\n",
    "        self.lr = nn.LeakyReLU(0.02)\n",
    "        self.drop = nn.Dropout(0.15)        \n",
    "    def forward(self, x):\n",
    "        x = self.lr(self.conv1_bn(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lr(self.conv2_bn(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import unlabeled data\n",
    "unlabeled = import_OM(\"unlabeled\")\n",
    "unlabeled = unlabeled[:,np.newaxis,:]\n",
    "ul_loader = get_dataloader_unlabeled(unlabeled,trsfrm, bs, shuffle=False)\n",
    "    \n",
    "def merge_into_training(train_data, train_label, new_labeled_data, new_train_label, shuffle = True):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train_data: n samples x 1 channel x 3751 dimensions, previously used training dataset\n",
    "    :param new_labeled_data: k samples x 1 channel x 3751 dimensions newly labeled training data\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_data = np.concatenate((train_data, new_labeled_data), axis=0)\n",
    "    train_label = np.concatenate((train_label, new_train_label), axis=0)\n",
    "\n",
    "    return train_data, train_label\n",
    "\n",
    "def evaluate_unlabeled(unlabeled_data, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Unlabeled data is not shufled when turned into tensor for evaluation\n",
    "    Making it easier to give it back to training at beginning of loop\n",
    "    :param unlabeled_data: data to be predicted\n",
    "    :param threshold: confidence threshold for accepting label as true\n",
    "    :return: new_labeled_data\n",
    "    \"\"\"\n",
    "def get_dataloader_unlabeled(X,transform, batch_size, shuffle=True, task_type = \"Regression\"):\n",
    "    \n",
    "    if task_type not in [\"Regression\", \"Classification\" , \"Ranking\"]:\n",
    "        raise ValueError(\"task_type must be in ['Regression', 'Classification' , 'Ranking']\")\n",
    "\n",
    "    if task_type == \"Ranking\":\n",
    "        dataset = Rank_dataset(X, y, transform=transform)\n",
    "    elif task_type == \"Classification\":\n",
    "        # X is n,1,3750, we want to calculate the FFT over the last axis\n",
    "        arr_copy = np.copy(X)\n",
    "        arr_copy = np.fft.rfft(arr_copy, axis=2).astype(np.float32)\n",
    "        dataset = OM_dataset(arr_copy, y, transform=transform)    \n",
    "    else:\n",
    "        placeholder = np.zeros([np.shape(X[:,:,])[0],1])\n",
    "        y = placeholder;\n",
    "        dataset = OM_dataset(X, y, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=0)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if task_type not in [\"Regression\", \"Classification\", \"Ranking\"]:\n",
    "    raise ValueError(\"task_type must be in ['Regression', 'Classification' , 'Ranking']\")\n",
    "\n",
    "# Set up losses and accuracies\n",
    "\n",
    "count = 0\n",
    "good_ent = []\n",
    "bad_ent = []\n",
    "bad_out = []\n",
    "bad_ent_h = []\n",
    "good_ent_h = []\n",
    "good_out = []\n",
    "for epoch in range(50):\n",
    "    # Training\n",
    "    model.eval()\n",
    "\n",
    "    for s, (input, labels) in enumerate(valid_loader):\n",
    "        data = input.float().to(device)\n",
    "        X = preprocess(data.cpu()).numpy()\n",
    "        fft = torch.Tensor(np.fft.rfft(X, axis = 2).astype(np.float32)).to(device)\n",
    "        noise1 = torch.randn_like(data)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(data,fft)\n",
    "        outputs = outputs.exp()\n",
    "        xx = F.softmax(outputs,dim=1)\n",
    "        [p,l]=outputs.max(dim=1)\n",
    "        ent_vals= []\n",
    "        ent_vals_h= []\n",
    "        for i in np.arange(np.shape(data)[0]):\n",
    "            out = outputs[i]\n",
    "            out = out[np.newaxis,:]\n",
    "            e =entropy_term(out,True)\n",
    "            e_h_o = HLoss()\n",
    "            e_h = e_h_o(out)\n",
    "            ent_vals.append(e.item())\n",
    "            ent_vals_h.append(e_h.item())\n",
    "\n",
    "            if labels[i].item()==l[i].item():\n",
    "                print(i,p[i].item(),ent_vals[i])\n",
    "                good_ent.append(ent_vals[i])\n",
    "                good_out.append(p[i].item())\n",
    "                good_ent_h.append(ent_vals_h[i])\n",
    "                count = count+1\n",
    "            else:\n",
    "                bad_ent.append(ent_vals[i])\n",
    "                bad_out.append(p[i].item())\n",
    "                bad_ent_h.append(ent_vals_h[i])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(good_ent)/len(bad_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_maxexp,count_minexp,count_min,count_max,count_sm_max, count_sm_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(np.shape(input)[0]):\n",
    "    out = outputs[i]\n",
    "    out = out[np.newaxis,:]\n",
    "    e =entropy_term(out,True)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last, unlabeled = unlabeled[-1], unlabeled[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl = np.arange(np.min(bad_ent),np.max(bad_ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(bad_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xo = np.arange(3750)\n",
    "\n",
    "\n",
    "ent_values = [];\n",
    "ent_values_h = [];\n",
    "s_m_values = [];\n",
    "loss_values = [];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(sns.color_palette(\"Greens\", 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(acc_values,ent_values)\n",
    "plt.plot(good_out)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(acc_values,ent_values)\n",
    "plt.hist(bad_ent_h,1000)\n",
    "plt.hist(good_ent_h,1000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bad_ent_h<min(bad_ent_h[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bj = [i for i in bad_out if i >= max(bad_out[:1000])]\n",
    "gj = [i for i in good_ent_h if i <= min(bad_ent_h)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
